{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "0n4asy4U_07i"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage import io\n",
    "\n",
    "from matplotlib import colors, pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdEp2js6_07j"
   },
   "source": [
    "Фиксируем все сиды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26NsfaaC_07j"
   },
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiJ762TI_07j"
   },
   "outputs": [],
   "source": [
    "# разные режимы датасета \n",
    "DATA_MODES = ['train', 'val', 'test']\n",
    "# все изображения будут масштабированы к размеру 224x224 px\n",
    "RESCALE_SIZE = 224\n",
    "# работаем на видеокарте\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23bvFdGA_07j"
   },
   "source": [
    "Берем бейзлайн с кагла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57jdCfIL_07j"
   },
   "outputs": [],
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с картинками, который паралельно подгружает их из папок\n",
    "    производит скалирование и превращение в торчевые тензоры\n",
    "    \"\"\"\n",
    "    def __init__(self, files, mode):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "     \n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file)\n",
    "        image.load()\n",
    "        return image\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
    "        # для тренировки добавим аугов, судя по бегому просмотру яркость, и контраст \n",
    "        # значитьльно разнится. Также добавим стандатрные отражения и перевороты. Уберем с бейзлайна \n",
    "        # повороты в 30* и ресайз.\n",
    "        if self.mode == 'train':\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomChoice(\n",
    "                                  [transforms.ColorJitter(0.5, 0.5),\n",
    "                                  transforms.RandomPerspective(),\n",
    "                                  transforms.RandomGrayscale(),\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.RandomRotation(180)]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "                                           ])\n",
    "        else:         \n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "            ])\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = self._prepare_sample(x)\n",
    "        x = transform(x)\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "        \n",
    "    def _prepare_sample(self, image):\n",
    "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
    "        return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT3m33oN_07j"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
    "    \"\"\"Imshow для тензоров\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt_ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt_ax.set_title(title)\n",
    "    plt_ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEldqTNF_07j"
   },
   "source": [
    "Дополним классы у которых мало картинок, до >=500 картинок в классе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNZ-C42l_07j",
    "outputId": "d2f0aec4-4cd2-4f25-82c6-760c09688fbd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "local_path = os.getcwd()\n",
    "\n",
    "TRAIN_DIR = Path(local_path+'\\\\train\\\\simpsons_dataset')\n",
    "TEST_DIR = Path(local_path+'\\\\testset\\\\testset')\n",
    "\n",
    "try:\n",
    "    shutil.copytree(TRAIN_DIR, 'working\\\\simpsons_dataset')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "    \n",
    "TRAIN_DIR = Path(local_path+'\\\\working\\\\simpsons_dataset')\n",
    "\n",
    "folders = ([name for name in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, name))])\n",
    "\n",
    "for folder in folders:\n",
    "    contents = os.listdir(os.path.join(TRAIN_DIR,folder))\n",
    "    while len(contents) < 500:\n",
    "        i = 1\n",
    "        for file in contents:\n",
    "            base, extension = os.path.splitext(file)\n",
    "            new_file = os.path.join(TRAIN_DIR,folder, base + str(i) + extension)\n",
    "            shutil.copy(os.path.join(TRAIN_DIR,folder, file), os.path.join(TRAIN_DIR,folder, new_file))\n",
    "        i += 1\n",
    "        contents = os.listdir(os.path.join(TRAIN_DIR,folder))\n",
    "    print(f\"{folder}\\t{len(contents)}\")\n",
    "\n",
    "              \n",
    "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
    "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8W7gizl_07j"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_labels = [path.parent.name for path in train_val_files]\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
    "                                          stratify=train_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRtwDaNj_07k"
   },
   "outputs": [],
   "source": [
    "val_dataset = SimpsonsDataset(val_files, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d63HBRoT_07k",
    "outputId": "721708c8-c10a-47f3-ba3b-4f95bc705abd"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n",
    "                        sharey=True, sharex=True)\n",
    "for fig_x in ax.flatten():\n",
    "    random_characters = int(np.random.uniform(0,1000))\n",
    "    im_val, label = val_dataset[random_characters]\n",
    "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
    "                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
    "    imshow(im_val.data.cpu(), \\\n",
    "          title=img_label,plt_ax=fig_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhVwVF0-_07k"
   },
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99bAo5Q9_07k"
   },
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDzOE0gU_07k"
   },
   "outputs": [],
   "source": [
    "def train(train_files, val_files, model, epochs, batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        # заменяем исходный lr, добавляем scheduler\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size = 14, gamma=0.1)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
    "            scheduler.step()\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuVr7kQq_07k"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K46i57wk_07k",
    "outputId": "7025e82f-d48f-412e-ae44-153b8a91a161",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(train_val_labels))\n",
    "print(\"We will classify {} classes\".format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8w_IfCk_07k"
   },
   "outputs": [],
   "source": [
    "if val_dataset is None:\n",
    "    val_dataset = SimpsonsDataset(val_files, mode='val')\n",
    "    \n",
    "train_dataset = SimpsonsDataset(train_files, mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J3FBsm7_07k"
   },
   "source": [
    "Используем Pretreined Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nju8d0Co_07k",
    "outputId": "54b43fc2-6da4-4086-bd76-46dcdc187009"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, n_classes)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6VQxU_j_07l"
   },
   "source": [
    "Запускаем обучение сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vswQpPBk_07l",
    "outputId": "b760ac96-4329-47ef-a9dd-3b0772f7abff",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = train(train_dataset, val_dataset, model=model, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_HICps9_07l"
   },
   "source": [
    "Построим кривые обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfKViGmF_07l"
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8PmIzrY_07l",
    "outputId": "9bc6db51-4be7-456b-ba4b-615bcc0aa090"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I-wvcsH_07l"
   },
   "outputs": [],
   "source": [
    "idxs = list(map(int, np.random.uniform(0,1000, 50)))\n",
    "imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n",
    "\n",
    "probs_ims = predict(model, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnrLZT0H_07l"
   },
   "outputs": [],
   "source": [
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPuiXzU2_07l"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(probs_ims,-1)\n",
    "\n",
    "actual_labels = [val_dataset[id][1] for id in idxs]\n",
    "\n",
    "preds_class = [i for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjvaRXHp_07l",
    "outputId": "1ccc2b81-b79d-4279-b093-2872ea269cda"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(actual_labels, preds_class, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9OFLZig_07l"
   },
   "outputs": [],
   "source": [
    "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
    "probs = predict(model, test_loader)\n",
    "\n",
    "\n",
    "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "test_filenames = [path.name for path in test_dataset.files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHysZW28_07l",
    "outputId": "84ea8b12-4004-4c0a-9aea-fc6b069d3fcb"
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlqyuJ9x_07l"
   },
   "outputs": [],
   "source": [
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsxUPiK-AIu5"
   },
   "source": [
    "Итого: F1 - 0.99893\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Simpsons",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
